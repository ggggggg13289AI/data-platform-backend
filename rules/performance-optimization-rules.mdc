---
description: FastAPI 應用程式效能最佳化規則，包括非同步操作和快取策略
globs: "**/*.py"
priority: high
category: performance
alwaysApply: true
dependencies: ["database-interaction-rules.mdc", "fastapi-application-rules.mdc"]
---

# 效能最佳化規則

## 核心原則
- **非同步優先**: 最小化阻塞 I/O 操作，所有資料庫和外部 API 請求使用非同步操作
- **智能快取**: 對靜態和頻繁存取的資料實施快取策略
- **資料最佳化**: 最佳化 Pydantic 的資料序列化和反序列化
- **延遲載入**: 對大型資料集和大量 API 回應使用延遲載入技術
- **效能監控**: 優先考慮 API 效能指標（回應時間、延遲、吞吐量）

## 非同步操作模式

### 並行資料庫查詢
```python
import asyncio
from sqlalchemy.ext.asyncio import AsyncSession

# ✅ 並行執行多個查詢
async def get_user_dashboard_data(
    db: AsyncSession, user_id: int
) -> DashboardData:
    """並行取得使用者儀表板資料"""
    # 並行執行多個查詢
    user_task = asyncio.create_task(get_user_by_id(db, user_id))
    posts_task = asyncio.create_task(get_user_posts(db, user_id, limit=10))
    stats_task = asyncio.create_task(get_user_statistics(db, user_id))
    
    # 等待所有查詢完成
    user, posts, stats = await asyncio.gather(
        user_task, posts_task, stats_task
    )
    
    return DashboardData(user=user, recent_posts=posts, statistics=stats)
```

### 批次處理最佳化
```python
# ✅ 限制並行數量的批次處理
async def process_multiple_files(file_ids: List[int]) -> List[ProcessingResult]:
    """批次處理多個檔案"""
    semaphore = asyncio.Semaphore(5)  # 最多同時處理 5 個檔案
    
    async def process_with_limit(file_id):
        async with semaphore:
            return await process_single_file(file_id)
    
    results = await asyncio.gather(
        *[process_with_limit(file_id) for file_id in file_ids],
        return_exceptions=True
    )
    
    return results
```

## 快取策略

### Redis 快取實作
```python
import redis.asyncio as redis
import pickle
from functools import wraps

class CacheManager:
    """Redis 快取管理器"""
    
    def __init__(self, redis_url: str):
        self.redis = redis.from_url(redis_url)
    
    async def get(self, key: str):
        """取得快取值"""
        try:
            value = await self.redis.get(key)
            return pickle.loads(value) if value else None
        except Exception as e:
            logger.error(f"快取讀取失敗: {e}")
            return None
    
    async def set(self, key: str, value, expire: int = 300):
        """設定快取值"""
        try:
            serialized_value = pickle.dumps(value)
            await self.redis.setex(key, expire, serialized_value)
            return True
        except Exception as e:
            logger.error(f"快取寫入失敗: {e}")
            return False

# ✅ 快取裝飾器
def cached(expire: int = 300, key_prefix: str = ""):
    """快取裝飾器"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            cache_key = f"{key_prefix}:{func.__name__}:{hash(str(args) + str(kwargs))}"
            
            # 檢查快取
            cached_result = await cache_manager.get(cache_key)
            if cached_result is not None:
                return cached_result
            
            # 執行函數並快取結果
            result = await func(*args, **kwargs)
            await cache_manager.set(cache_key, result, expire)
            
            return result
        return wrapper
    return decorator
```

### 記憶體快取
```python
from functools import lru_cache
import time
from typing import Dict, Any, Optional

# ✅ LRU 快取（同步函數）
@lru_cache(maxsize=1000)
def get_medical_constants(constant_type: str) -> Dict[str, Any]:
    """取得醫學常數（記憶體快取）"""
    return load_medical_constants(constant_type)

# ✅ 非同步記憶體快取
class AsyncMemoryCache:
    """非同步記憶體快取"""
    
    def __init__(self, max_size: int = 1000, ttl: int = 300):
        self.cache: Dict[str, tuple] = {}  # (value, timestamp)
        self.max_size = max_size
        self.ttl = ttl
    
    async def get(self, key: str) -> Optional[Any]:
        """取得快取值"""
        if key in self.cache:
            value, timestamp = self.cache[key]
            if time.time() - timestamp < self.ttl:
                return value
            else:
                del self.cache[key]
        return None
    
    async def set(self, key: str, value: Any) -> None:
        """設定快取值"""
        if len(self.cache) >= self.max_size:
            # 移除最舊的項目
            oldest_key = min(self.cache.keys(), key=lambda k: self.cache[k][1])
            del self.cache[oldest_key]
        
        self.cache[key] = (value, time.time())
```

## 資料序列化最佳化

### Pydantic 最佳化
```python
from pydantic import BaseModel, Field
import orjson

# ✅ 高效能 Pydantic 模型
class OptimizedUserResponse(BaseModel):
    """最佳化的使用者回應模型"""
    id: int
    email: str
    name: str
    is_active: bool
    created_at: datetime
    
    class Config:
        # 使用 orjson 提升序列化效能
        json_loads = orjson.loads
        json_dumps = orjson.dumps
        
        # 最佳化設定
        allow_reuse = True
        validate_assignment = False
        use_enum_values = True
        orm_mode = True
```

## 延遲載入技術

### 選擇性預載入
```python
from sqlalchemy.orm import selectinload, joinedload
from sqlalchemy import select

# ✅ 選擇性預載入
async def get_user_with_selective_loading(
    db: AsyncSession, 
    user_id: int,
    include_posts: bool = False,
    include_profile: bool = False
) -> Optional[User]:
    """選擇性預載入使用者資料"""
    query = select(User).where(User.id == user_id)
    
    if include_posts:
        query = query.options(selectinload(User.posts))
    
    if include_profile:
        query = query.options(joinedload(User.profile))
    
    result = await db.execute(query)
    return result.scalar_one_or_none()
```

## 效能監控

### 效能指標收集
```python
import time
from functools import wraps
from typing import Dict, List

class PerformanceMonitor:
    """效能監控器"""
    
    def __init__(self):
        self.metrics: Dict[str, List[float]] = {}
    
    def record_metric(self, name: str, duration: float):
        """記錄效能指標"""
        if name not in self.metrics:
            self.metrics[name] = []
        
        self.metrics[name].append(duration)
        
        # 保持最近 1000 筆記錄
        if len(self.metrics[name]) > 1000:
            self.metrics[name] = self.metrics[name][-1000:]
    
    def get_stats(self, name: str) -> Dict[str, float]:
        """取得統計資料"""
        if name not in self.metrics or not self.metrics[name]:
            return {}
        
        durations = self.metrics[name]
        return {
            "count": len(durations),
            "avg": sum(durations) / len(durations),
            "min": min(durations),
            "max": max(durations)
        }

# ✅ 效能監控裝飾器
def monitor_performance(name: str = None):
    """效能監控裝飾器"""
    def decorator(func):
        metric_name = name or f"{func.__module__}.{func.__name__}"
        
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            
            try:
                result = await func(*args, **kwargs)
                return result
            finally:
                duration = time.time() - start_time
                performance_monitor.record_metric(metric_name, duration)
        
        return wrapper
    return decorator
```

## 連接池最佳化

### 資料庫連接池
```python
from sqlalchemy.ext.asyncio import create_async_engine
from sqlalchemy.pool import QueuePool

# ✅ 最佳化的資料庫引擎
def create_optimized_engine(database_url: str):
    """建立最佳化的資料庫引擎"""
    return create_async_engine(
        database_url,
        poolclass=QueuePool,
        pool_size=20,           # 連接池大小
        max_overflow=30,        # 最大溢出連接
        pool_pre_ping=True,     # 連接前測試
        pool_recycle=3600,      # 連接回收時間（1小時）
        echo=False,             # 生產環境關閉 SQL 日誌
        future=True,            # 使用 2.0 風格
    )
```

## 禁止的效能反模式

### ❌ 絕對不要使用
```python
# ❌ 同步阻塞操作
def sync_database_call():
    # 在非同步環境中使用同步資料庫呼叫
    pass

# ❌ N+1 查詢問題
async def bad_get_users_with_posts():
    users = await get_all_users()
    for user in users:
        user.posts = await get_posts_by_user_id(user.id)  # N+1 問題
    return users

# ❌ 無限制的並行操作
async def bad_unlimited_concurrent():
    tasks = [process_item(item) for item in huge_list]
    # 無限制並行，可能耗盡資源
    await asyncio.gather(*tasks)
```